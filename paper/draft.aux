\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{xie2024mhc}
\citation{trask2018neural}
\citation{dong2019neural}
\citation{riegel2020logical}
\citation{serafini2016learning}
\citation{odonnell2014analysis}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{linial1993constant}
\citation{xie2024mhc}
\citation{he2016identity}
\citation{sinkhorn1967concerning}
\citation{odonnell2014analysis}
\@writefile{toc}{\contentsline {paragraph}{Boolean Fourier Analysis.}{2}{section*.1}\protected@file@percent }
\newlabel{eq:fourier}{{1}{2}{Boolean Fourier Analysis}{equation.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Polynomial Threshold Representations.}{2}{section*.2}\protected@file@percent }
\newlabel{eq:ptf}{{2}{2}{Polynomial Threshold Representations}{equation.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Connection to \textit  {m}HC.}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Start with $n=2$?}{2}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Hardware Motivation.}{2}{section*.5}\protected@file@percent }
\citation{xie2024mhc}
\citation{xie2024mhc}
\citation{zhu2024hc}
\citation{sinkhorn1967concerning}
\citation{cuturi2013sinkhorn}
\@writefile{toc}{\contentsline {paragraph}{Explainability and Transparent-by-Design AI.}{3}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contributions.}{3}{section*.7}\protected@file@percent }
\citation{mena2018learning}
\citation{sander2022sinkformers}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Manifold-Constrained Routing: The \textit  {m}HC{} Foundation}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Our Extension.}{4}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Related Optimal Transport Approaches.}{4}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Recent Differentiable Logic Gate Networks (2024-2025)}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{WARP-LUTs}{4}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mind the Gap}{4}{section*.11}\protected@file@percent }
\citation{linial1993constant}
\citation{kushilevitz1993learning}
\citation{odonnell2014analysis}
\citation{pan2021wht}
\citation{daniely2020learning}
\citation{trask2018neural}
\citation{dong2019neural}
\citation{riegel2020logical}
\citation{petersen2022deep}
\@writefile{toc}{\contentsline {paragraph}{Convolutional DLGNs}{5}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Spectral Learning of Boolean Functions}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recent Theoretical Advances.}{5}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral Bias and Regularization.}{5}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral Methods for Interpretability.}{5}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Our Contribution.}{5}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Neural Logic and Neuro-Symbolic Systems}{5}{subsection.2.4}\protected@file@percent }
\citation{odonnell2014analysis}
\citation{courbariaux2016binarized}
\citation{zhu2017trained}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Transparent-by-Design Architectures}{6}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretable Features.}{6}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Concept Bottleneck Models.}{6}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mechanistic Interpretability.}{6}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Our Contribution.}{6}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Quantization and Efficient Inference}{6}{subsection.2.6}\protected@file@percent }
\citation{odonnell2014analysis}
\citation{xie2024mhc}
\citation{sinkhorn1967concerning}
\citation{xie2024mhc}
\citation{cuturi2013sinkhorn}
\citation{mena2018learning}
\@writefile{toc}{\contentsline {section}{\numberline {3}Preliminaries}{7}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Boolean Fourier Analysis}{7}{subsection.3.1}\protected@file@percent }
\newlabel{prop:completeness}{{1}{7}{Completeness}{proposition.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Birkhoff Polytope and Sinkhorn Projection}{7}{subsection.3.2}\protected@file@percent }
\newlabel{remark:rectangular}{{1}{7}{Rectangular Sinkhorn}{remark.1}{}}
\newlabel{prop:sinkhorn_convergence}{{2}{7}{Sinkhorn Convergence to Permutation}{proposition.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Hierarchical Spectral Composition architecture. Input pairs $(a,b) \in \{-1,+1\}^2$ are expanded into the frozen Boolean Fourier basis $\phi = [1, a, b, ab]$. Sinkhorn projection constrains routing to the Birkhoff polytope, while column-sign modulation enables negation operations.}}{8}{figure.caption.21}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:architecture}{{1}{8}{Hierarchical Spectral Composition architecture. Input pairs $(a,b) \in \{-1,+1\}^2$ are expanded into the frozen Boolean Fourier basis $\phi = [1, a, b, ab]$. Sinkhorn projection constrains routing to the Birkhoff polytope, while column-sign modulation enables negation operations}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method: Hierarchical Spectral Composition}{8}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Phase 1: Spectral Coefficient Selection}{8}{subsection.4.1}\protected@file@percent }
\newlabel{sec:phase1}{{4.1}{8}{Phase 1: Spectral Coefficient Selection}{subsection.4.1}{}}
\citation{odonnell2014analysis}
\citation{jang2017categorical}
\citation{maddison2017concrete}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Architecture}{9}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}The Dynamics of Parity Selection}{9}{subsubsection.4.1.2}\protected@file@percent }
\newlabel{prop:parity}{{3}{9}{Parity Gradient Accumulation}{proposition.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Optimization Scaffolding}{9}{subsubsection.4.1.3}\protected@file@percent }
\newlabel{remark:selection}{{2}{9}{``Selection'' vs. ``Discovery''}{remark.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Training Methodology: Gumbel-Softmax Ternary Relaxation}{9}{subsubsection.4.1.4}\protected@file@percent }
\newlabel{sec:phase1_training}{{4.1.4}{9}{Training Methodology: Gumbel-Softmax Ternary Relaxation}{subsubsection.4.1.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Gumbel-Softmax Parameterization.}{9}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sequential Training Protocol.}{10}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ternary Attractor Regularization.}{10}{section*.24}\protected@file@percent }
\newlabel{box:encoding}{{4.1.4}{10}{Ternary Attractor Regularization}{equation.4.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5}Validation Suite}{10}{subsubsection.4.1.5}\protected@file@percent }
\newlabel{sec:phase1_validation}{{4.1.5}{10}{Validation Suite}{subsubsection.4.1.5}{}}
\citation{xie2024mhc}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Phase 1 Learned Ternary Masks ($n=2$). All four operations achieve 100\% accuracy. Encoding: $-1 = \text  {TRUE}$, $+1 = \text  {FALSE}$.}}{11}{table.caption.25}\protected@file@percent }
\newlabel{tab:phase1_masks}{{1}{11}{Phase 1 Learned Ternary Masks ($n=2$). All four operations achieve 100\% accuracy. Encoding: $-1 = \text {TRUE}$, $+1 = \text {FALSE}$}{table.caption.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.6}Phase 1 Results}{11}{subsubsection.4.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Dynamics.}{11}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Validation Results.}{11}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Phase 2: Sinkhorn-Constrained Composition with Column-Sign Modulation}{11}{subsection.4.2}\protected@file@percent }
\newlabel{sec:phase2}{{4.2}{11}{Phase 2: Sinkhorn-Constrained Composition with Column-Sign Modulation}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}The Expressivity Problem: Why Standard \textit  {m}HC{} Is Insufficient}{11}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{prop:negation}{{4}{11}{Negation Inaccessibility in Doubly Stochastic Routing}{proposition.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Phase 1 training dynamics for all four base operations. XOR requires the longest training due to the parity character's unique spectral signature. AND and OR converge in $<500$ steps due to their simpler affine structure.}}{12}{figure.caption.27}\protected@file@percent }
\newlabel{fig:phase1_training}{{2}{12}{Phase 1 training dynamics for all four base operations. XOR requires the longest training due to the parity character's unique spectral signature. AND and OR converge in $<500$ steps due to their simpler affine structure}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Column-Sign Modulation: Extending \textit  {m}HC}{12}{subsubsection.4.2.2}\protected@file@percent }
\citation{xie2024mhc}
\citation{he2016identity}
\citation{xie2024mhc}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces XOR parity emergence: the $|c_{ab}|$ coefficient grows from noise to 1.0 while other coefficients ($|c_1|$, $|c_a|$, $|c_b|$) decay to zero. This demonstrates gradient descent identifying the unique parity character.}}{13}{figure.caption.28}\protected@file@percent }
\newlabel{fig:xor_emergence}{{3}{13}{XOR parity emergence: the $|c_{ab}|$ coefficient grows from noise to 1.0 while other coefficients ($|c_1|$, $|c_a|$, $|c_b|$) decay to zero. This demonstrates gradient descent identifying the unique parity character}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {paragraph}{Sign Learning.}{13}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Identity Initialization: Adapting \textit  {m}HC{} Insights}{13}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Motivation for Identity Initialization.}{13}{section*.31}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Phase 2A: Linear Operations (8 ops, 10 seeds). All metrics achieve perfect scores.}}{14}{table.caption.33}\protected@file@percent }
\newlabel{tab:phase2a}{{2}{14}{Phase 2A: Linear Operations (8 ops, 10 seeds). All metrics achieve perfect scores}{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Quantization: From Soft to Hard Routing}{14}{subsection.4.3}\protected@file@percent }
\newlabel{sec:quantization}{{4.3}{14}{Quantization: From Soft to Hard Routing}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantization Statistics.}{14}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Phase 2A Validation: Linear Operations}{14}{subsubsection.4.3.1}\protected@file@percent }
\newlabel{sec:phase2a_validation}{{4.3.1}{14}{Phase 2A Validation: Linear Operations}{subsubsection.4.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Validation Metrics (10 seeds):}{14}{section*.34}\protected@file@percent }
\citation{xie2024mhc}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Phase 2 routing visualization. Left: Sinkhorn-constrained $P$ learns identity routing. Middle: Column-sign $s$ enables negation (ops 4-7). Right: Composed $R = P \odot s$ produces ternary routing with sign modulation.}}{15}{figure.caption.35}\protected@file@percent }
\newlabel{fig:phase2_routing}{{4}{15}{Phase 2 routing visualization. Left: Sinkhorn-constrained $P$ learns identity routing. Middle: Column-sign $s$ enables negation (ops 4-7). Right: Composed $R = P \odot s$ produces ternary routing with sign modulation}{figure.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Phase 2: Nonlinear Operations (8 ops). Each has a valid ternary mask but requires direct learning, not routing.}}{15}{table.caption.36}\protected@file@percent }
\newlabel{tab:phase2_nonlinear}{{3}{15}{Phase 2: Nonlinear Operations (8 ops). Each has a valid ternary mask but requires direct learning, not routing}{table.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Phase 2 Full: All 16 Operations Analysis}{15}{subsubsection.4.3.2}\protected@file@percent }
\newlabel{sec:phase2_full}{{4.3.2}{15}{Phase 2 Full: All 16 Operations Analysis}{subsubsection.4.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Finding: Routing Expressibility Boundary.}{15}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sparsity Analysis.}{15}{section*.38}\protected@file@percent }
\citation{xie2024mhc}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Phase 2 Validation Results ($n=2$, 10 Seeds). The ``No Sign Mod.'' ablation corresponds to pure \textit  {m}HC-style doubly stochastic routing, which caps at 75\% (12/16 operations)---confirming Proposition~\ref {prop:negation}.}}{16}{table.caption.41}\protected@file@percent }
\newlabel{tab:main_results}{{4}{16}{Phase 2 Validation Results ($n=2$, 10 Seeds). The ``No Sign Mod.'' ablation corresponds to pure \mHC -style doubly stochastic routing, which caps at 75\% (12/16 operations)---confirming Proposition~\ref {prop:negation}}{table.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{16}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experimental Setup}{16}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation.}{16}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablation Testing Protocol.}{16}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Phase 2 Results ($n=2$): Architecture Validation}{16}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diagnostic: Sign-Only Learning.}{16}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpreting Ablation Results.}{16}{section*.43}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Ternary Masks for All 16 Boolean Operations ($n=2$). Encoding: $-1 = \text  {TRUE}$, $+1 = \text  {FALSE}$. Each mask yields the correct truth table: $f(a,b) = \text  {sign}(c_0 + c_a \cdot a + c_b \cdot b + c_{ab} \cdot ab)$.}}{17}{table.caption.44}\protected@file@percent }
\newlabel{tab:ternary}{{5}{17}{Ternary Masks for All 16 Boolean Operations ($n=2$). Encoding: $-1 = \text {TRUE}$, $+1 = \text {FALSE}$. Each mask yields the correct truth table: $f(a,b) = \sign (c_0 + c_a \cdot a + c_b \cdot b + c_{ab} \cdot ab)$}{table.caption.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Constructive Ternary Representability ($n=2$)}{17}{subsection.5.3}\protected@file@percent }
\newlabel{thm:ternary}{{1}{17}{Ternary Representability for $n=2$}{theorem.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Phase 3: Three-Variable Operations ($n=3$)}{17}{section.6}\protected@file@percent }
\newlabel{sec:phase3}{{6}{17}{Phase 3: Three-Variable Operations ($n=3$)}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Architecture and Target Operations}{18}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Representability Analysis}{18}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Exhaustive Enumeration.}{18}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Finding: Universal Representability.}{18}{section*.46}\protected@file@percent }
\newlabel{thm:ternary_n3}{{2}{18}{Universal Ternary Representability for $n=3$}{theorem.2}{}}
\newlabel{thm:ternary_n4}{{3}{18}{Universal Ternary Representability for $n=4$}{theorem.3}{}}
\@writefile{toc}{\contentsline {paragraph}{NPN Equivalence Classes.}{19}{section*.47}\protected@file@percent }
\newlabel{conj:ternary}{{1}{19}{Universal Ternary Representability}{conjecture.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Heuristic Search at $n=5$ and $n=6$.}{19}{section*.48}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Counterexample search results. ``Named'' = structured functions (majority, parity, threshold-$k$, tribes, etc.). ``Random'' = uniformly random truth tables. Failures indicate search limitations, not proven non-representability.}}{19}{table.caption.49}\protected@file@percent }
\newlabel{tab:counterexample}{{6}{19}{Counterexample search results. ``Named'' = structured functions (majority, parity, threshold-$k$, tribes, etc.). ``Random'' = uniformly random truth tables. Failures indicate search limitations, not proven non-representability}{table.caption.49}{}}
\citation{linial1993constant}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Phase 3 Results Summary ($n=3$, 5 Seeds)}}{20}{table.caption.51}\protected@file@percent }
\newlabel{tab:phase3_summary}{{7}{20}{Phase 3 Results Summary ($n=3$, 5 Seeds)}{table.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Phase 3 Ternary Masks (8-dimensional basis). All 10 operations achieve 100\% accuracy. Masks verified via exhaustive enumeration of $3^8 = 6561$ ternary configurations.}}{20}{table.caption.52}\protected@file@percent }
\newlabel{tab:phase3_masks}{{8}{20}{Phase 3 Ternary Masks (8-dimensional basis). All 10 operations achieve 100\% accuracy. Masks verified via exhaustive enumeration of $3^8 = 6561$ ternary configurations}{table.caption.52}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning vs. Optimal.}{20}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Results}{20}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Observations.}{20}{section*.53}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Phase 3 optimal ternary masks for all 10 three-variable operations. Colors indicate coefficient values: blue (+1), white (0), red (-1). Pure 3-var operations (top) vs. cascade compositions (bottom). The sparsity pattern (39\% zeros) reflects spectral concentration.}}{21}{figure.caption.54}\protected@file@percent }
\newlabel{fig:phase3_heatmap}{{5}{21}{Phase 3 optimal ternary masks for all 10 three-variable operations. Colors indicate coefficient values: blue (+1), white (0), red (-1). Pure 3-var operations (top) vs. cascade compositions (bottom). The sparsity pattern (39\% zeros) reflects spectral concentration}{figure.caption.54}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Inference Throughput (Phase 3, batch=100,000, bits=64). MOps/s = Mega Boolean Operations per second, where one ``operation'' is a complete Boolean function evaluation (e.g., computing AND$(a,b)$ for one input pair).}}{21}{table.caption.55}\protected@file@percent }
\newlabel{tab:benchmark}{{9}{21}{Inference Throughput (Phase 3, batch=100,000, bits=64). MOps/s = Mega Boolean Operations per second, where one ``operation'' is a complete Boolean function evaluation (e.g., computing AND$(a,b)$ for one input pair)}{table.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Benchmark Performance}{21}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Throughput Definition.}{21}{section*.56}\protected@file@percent }
\citation{swendsen1986replica}
\@writefile{toc}{\contentsline {section}{\numberline {7}Phase 4: Four-Variable Operations ($n=4$)}{22}{section.7}\protected@file@percent }
\newlabel{sec:phase4}{{7}{22}{Phase 4: Four-Variable Operations ($n=4$)}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Architecture and Basis}{22}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Spectral Synthesis Method}{22}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 1: Exact Walsh-Hadamard Transform.}{22}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 2: Ternary Quantization.}{22}{section*.58}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Phase 4 Results Summary ($n=4$, 5 Seeds)}}{23}{table.caption.60}\protected@file@percent }
\newlabel{tab:phase4_summary}{{10}{23}{Phase 4 Results Summary ($n=4$, 5 Seeds)}{table.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Phase 4 Ternary Masks (16-dimensional basis). All 10 operations achieve 100\% accuracy via spectral synthesis. Basis: $[1, d, c, cd, b, bd, bc, bcd, a, ad, ac, acd, ab, abd, abc, abcd]$.}}{23}{table.caption.61}\protected@file@percent }
\newlabel{tab:phase4_masks}{{11}{23}{Phase 4 Ternary Masks (16-dimensional basis). All 10 operations achieve 100\% accuracy via spectral synthesis. Basis: $[1, d, c, cd, b, bd, bc, bcd, a, ad, ac, acd, ab, abd, abc, abcd]$}{table.caption.61}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage 3: MCMC Refinement via Parallel Tempering.}{23}{section*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Target Operations}{23}{subsection.7.3}\protected@file@percent }
\citation{kushilevitz1993learning}
\citation{xie2024mhc}
\citation{xie2024mhc}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Phase 4 synthesized ternary masks for all 10 four-variable operations. Colors indicate coefficient values: blue (+1), white (0), red (-1). Pure 4-var operations (top) vs. cascade compositions (bottom). XOR operations exhibit maximum sparsity (support=1).}}{24}{figure.caption.63}\protected@file@percent }
\newlabel{fig:phase4_heatmap}{{6}{24}{Phase 4 synthesized ternary masks for all 10 four-variable operations. Colors indicate coefficient values: blue (+1), white (0), red (-1). Pure 4-var operations (top) vs. cascade compositions (bottom). XOR operations exhibit maximum sparsity (support=1)}{figure.caption.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Results}{24}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Observations.}{24}{section*.62}\protected@file@percent }
\newlabel{cor:sparsity}{{1}{24}{Spectral Sparsity at Scale}{corollary.1}{}}
\citation{linial1993constant}
\citation{kushilevitz1993learning}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Spectral synthesis pipeline for Phase 4. Stage 1: Exact Walsh-Hadamard Transform for coefficient computation. Stage 2: Ternary quantization. Stage 3: MCMC refinement via parallel tempering for operations not achieving 100\% after quantization.}}{25}{figure.caption.64}\protected@file@percent }
\newlabel{fig:phase4_pipeline}{{7}{25}{Spectral synthesis pipeline for Phase 4. Stage 1: Exact Walsh-Hadamard Transform for coefficient computation. Stage 2: Ternary quantization. Stage 3: MCMC refinement via parallel tempering for operations not achieving 100\% after quantization}{figure.caption.64}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Comparison with \textit  {m}HC{} \cite  {xie2024mhc}. Our work adapts \textit  {m}HC's stability mechanisms to a new domain while adding column-sign modulation for Boolean expressivity.}}{25}{table.caption.65}\protected@file@percent }
\newlabel{tab:mhc_comparison}{{12}{25}{Comparison with \mHC {} \cite {xie2024mhc}. Our work adapts \mHC 's stability mechanisms to a new domain while adding column-sign modulation for Boolean expressivity}{table.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{25}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Relationship to \textit  {m}HC: Similarities and Differences}{25}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Scaling Analysis}{25}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Hardware Deployment}{25}{subsection.8.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Scaling Across Phases. All phases achieve 100\% accuracy. Sparsity varies based on operation complexity.}}{26}{table.caption.66}\protected@file@percent }
\newlabel{tab:scaling}{{13}{26}{Scaling Across Phases. All phases achieve 100\% accuracy. Sparsity varies based on operation complexity}{table.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Spectral sparsity across phases. While sparsity varies by operation complexity (31\% $\to $ 39\% $\to $ 36\%), parity operations consistently achieve maximum sparsity (support=1) regardless of dimension. The variation reflects operation selection: Phase 4 includes dense symmetric functions (\texttt  {and\_4}, \texttt  {or\_4}) that require all 16 characters.}}{26}{figure.caption.67}\protected@file@percent }
\newlabel{fig:sparsity}{{8}{26}{Spectral sparsity across phases. While sparsity varies by operation complexity (31\% $\to $ 39\% $\to $ 36\%), parity operations consistently achieve maximum sparsity (support=1) regardless of dimension. The variation reflects operation selection: Phase 4 includes dense symmetric functions (\texttt {and\_4}, \texttt {or\_4}) that require all 16 characters}{figure.caption.67}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Phase 5: Scalable Spectral Methods}{26}{section.9}\protected@file@percent }
\newlabel{sec:phase5}{{9}{26}{Phase 5: Scalable Spectral Methods}{section.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Track 1: Exact FWHT (Moderate $n$)}{26}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Result.}{26}{section*.69}\protected@file@percent }
\citation{linial1993constant}
\citation{goldreich1989hard}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Exact FWHT Benchmark (GPU, RTX 5060 8GB). Peak throughput of 1.64B coefficients/sec achieved at $n=27$. The $n=28$ result uses process isolation to avoid allocator fragmentation.}}{27}{table.caption.68}\protected@file@percent }
\newlabel{tab:fwht}{{14}{27}{Exact FWHT Benchmark (GPU, RTX 5060 8GB). Peak throughput of 1.64B coefficients/sec achieved at $n=27$. The $n=28$ result uses process isolation to avoid allocator fragmentation}{table.caption.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Track 2: Oracle Learning with Spectral Filtering}{27}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Method 1: Monte Carlo Baseline.}{27}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Method 2: Goldreich-Levin (GL).}{27}{section*.71}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Method 3--5: Spectral Filtering.}{27}{section*.72}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental Design.}{27}{section*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results.}{28}{section*.74}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Oracle Learning: Comparative Method Accuracy ($n=16$, degree $\leq 3$)}}{28}{table.caption.75}\protected@file@percent }
\newlabel{tab:oracle_comparison}{{15}{28}{Oracle Learning: Comparative Method Accuracy ($n=16$, degree $\leq 3$)}{table.caption.75}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation for Explainability.}{28}{section*.76}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Track 3: Hierarchical Composition}{28}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Approach.}{28}{section*.77}\protected@file@percent }
\citation{yousefi2025mind}
\citation{yousefi2025mind}
\citation{yousefi2025mind}
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces Hierarchical Circuit Composition. All circuits achieve 100\% accuracy. Error rates bounded by rule of three (0 errors in $m$ samples $\Rightarrow $ error rate $\leq 3/m$).}}{29}{table.caption.78}\protected@file@percent }
\newlabel{tab:composition}{{16}{29}{Hierarchical Circuit Composition. All circuits achieve 100\% accuracy. Error rates bounded by rule of three (0 errors in $m$ samples $\Rightarrow $ error rate $\leq 3/m$)}{table.caption.78}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Claim.}{29}{section*.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Track 4: Routing Mechanism Comparison}{29}{subsection.9.4}\protected@file@percent }
\newlabel{sec:routing_comparison}{{9.4}{29}{Track 4: Routing Mechanism Comparison}{subsection.9.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {17}{\ignorespaces Routing Mechanism Comparison on Mode-Switching Task ($n=2$, 4 frozen primitives). Sinkhorn achieves the best accuracy--interpretability tradeoff. Gumbel-STE exhibits the discretization gap discussed in \cite  {yousefi2025mind}: hard routing commits early but cannot escape suboptimal assignments.}}{29}{table.caption.80}\protected@file@percent }
\newlabel{tab:routing_comparison}{{17}{29}{Routing Mechanism Comparison on Mode-Switching Task ($n=2$, 4 frozen primitives). Sinkhorn achieves the best accuracy--interpretability tradeoff. Gumbel-STE exhibits the discretization gap discussed in \cite {yousefi2025mind}: hard routing commits early but cannot escape suboptimal assignments}{table.caption.80}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Findings.}{29}{section*.81}\protected@file@percent }
\citation{yousefi2025mind}
\citation{odonnell2014analysis}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.1}Spectral Analysis of Routing Dynamics}{30}{subsubsection.9.4.1}\protected@file@percent }
\newlabel{sec:spectral_analysis}{{9.4.1}{30}{Spectral Analysis of Routing Dynamics}{subsubsection.9.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Observation 1: Routing Uncertainty Principle.}{30}{section*.83}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {18}{\ignorespaces Spectral trajectory of routing matrix $P$ during training. Sinkhorn maintains a smooth, monotonic trajectory (gradual spectral redistribution). Gumbel-STE exhibits chaotic oscillations in $\Delta $, indicating unstable routing dynamics. ReinMax tracks Sinkhorn's trajectory as entropy regularization anneals to zero.}}{31}{table.caption.82}\protected@file@percent }
\newlabel{tab:spectral_trajectory}{{18}{31}{Spectral trajectory of routing matrix $P$ during training. Sinkhorn maintains a smooth, monotonic trajectory (gradual spectral redistribution). Gumbel-STE exhibits chaotic oscillations in $\Delta $, indicating unstable routing dynamics. ReinMax tracks Sinkhorn's trajectory as entropy regularization anneals to zero}{table.caption.82}{}}
\@writefile{toc}{\contentsline {paragraph}{Observation 2: CFL-Like Stability Condition.}{31}{section*.84}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {19}{\ignorespaces CFL Stability Experiment: accuracy and spectral diagnostics vs.\ annealing rate $r$ for Sinkhorn routing (lr $= 0.01$). A sharp transition between $r = 0.0012$ and $r = 0.0046$ separates stable convergence from failure.}}{31}{table.caption.85}\protected@file@percent }
\newlabel{tab:cfl_stability}{{19}{31}{CFL Stability Experiment: accuracy and spectral diagnostics vs.\ annealing rate $r$ for Sinkhorn routing (lr $= 0.01$). A sharp transition between $r = 0.0012$ and $r = 0.0046$ separates stable convergence from failure}{table.caption.85}{}}
\citation{kushilevitz1993learning}
\citation{xie2024mhc}
\@writefile{toc}{\contentsline {paragraph}{Observation 3: Spectral Margin and Adaptability.}{32}{section*.86}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implications for Transparent-by-Design AI.}{32}{section*.87}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Future Work}{32}{section.10}\protected@file@percent }
\newlabel{sec:future}{{10}{32}{Future Work}{section.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Conclusion}{32}{section.11}\protected@file@percent }
\bibstyle{plain}
\bibcite{xie2024mhc}{1}
\bibcite{zhu2024hc}{2}
\bibcite{odonnell2014analysis}{3}
\bibcite{linial1993constant}{4}
\bibcite{kushilevitz1993learning}{5}
\bibcite{goldreich1989hard}{6}
\@writefile{toc}{\contentsline {paragraph}{Explainability and Neurosymbolic Integration.}{33}{section*.88}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Role of Birkhoff Projection.}{33}{section*.89}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Toward Hardware-Efficient Neuro-Symbolic Systems.}{33}{section*.90}\protected@file@percent }
\bibcite{warp2025}{7}
\bibcite{yousefi2025gap}{8}
\bibcite{petersen2024cdlgn}{9}
\bibcite{barak2022hidden}{10}
\bibcite{gorji2023walsh}{11}
\bibcite{shoshani2025hardness}{12}
\bibcite{activation2025spectroscopy}{13}
\bibcite{xu2024frequency}{14}
\bibcite{kyrillidis2020fouriersat}{15}
\bibcite{novakovsky2023explainn}{16}
\bibcite{singh2025shallow}{17}
\bibcite{balci2023tisfm}{18}
\bibcite{xu2024energycbm}{19}
\bibcite{vandenhirtz2024stochastic}{20}
\bibcite{templeton2024scaling}{21}
\bibcite{cuturi2013sinkhorn}{22}
\bibcite{sinkhorn1967concerning}{23}
\bibcite{mena2018learning}{24}
\bibcite{sander2022sinkformers}{25}
\bibcite{trask2018neural}{26}
\bibcite{dong2019neural}{27}
\bibcite{serafini2016learning}{28}
\bibcite{riegel2020logical}{29}
\bibcite{petersen2022deep}{30}
\bibcite{daniely2020learning}{31}
\bibcite{pan2021wht}{32}
\bibcite{courbariaux2016binarized}{33}
\bibcite{zhu2017trained}{34}
\bibcite{jang2017categorical}{35}
\bibcite{maddison2017concrete}{36}
\bibcite{he2016identity}{37}
\bibcite{swendsen1986replica}{38}
\citation{xie2024mhc}
\@writefile{toc}{\contentsline {section}{\numberline {A}Implementation Details}{35}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn Iterations.}{35}{section*.92}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Temperature Annealing.}{35}{section*.93}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Plateau Detection.}{36}{section*.94}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Reproducibility}{36}{appendix.B}\protected@file@percent }
\gdef \@abspage@last{36}
